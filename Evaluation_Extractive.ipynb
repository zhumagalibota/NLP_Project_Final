{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NvVaiBma93r"
      },
      "outputs": [],
      "source": [
        "!pip install spacy rouge-score datasets tqdm scikit-learn bert-score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "id": "Oc79e1gqbFxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from heapq import nlargest\n",
        "from string import punctuation\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from rouge_score import rouge_scorer\n",
        "from tqdm import tqdm\n",
        "from bert_score import score"
      ],
      "metadata": {
        "id": "cqE24UfIbH07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds1 = load_dataset(\"EdinburghNLP/xsum\", split=\"test\")\n",
        "\n",
        "\n",
        "ds2 = load_dataset(\"ccdv/govreport-summarization\", split=\"test\")\n",
        "\n",
        "\n",
        "ds3 = load_dataset(\"abisee/cnn_dailymail\", \"1.0.0\", split=\"test\")\n",
        "\n",
        "\n",
        "ds4 = load_dataset(\"ccdv/pubmed-summarization\", \"document\", split=\"test\")\n"
      ],
      "metadata": {
        "id": "dKETLCHnbLIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Function**"
      ],
      "metadata": {
        "id": "zwcqZCm3bm3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "def summarize_text(text, compression_ratio=0.3):\n",
        "    try:\n",
        "        doc = nlp(text)\n",
        "        stopwords = list(STOP_WORDS)\n",
        "        word_frequencies = {}\n",
        "\n",
        "        for word in doc:\n",
        "            if word.text.lower() not in stopwords and word.text.lower() not in punctuation:\n",
        "                word_frequencies[word.text.lower()] = word_frequencies.get(word.text.lower(), 0) + 1\n",
        "\n",
        "        max_frequency = max(word_frequencies.values(), default=1)\n",
        "        word_frequencies = {word: freq / max_frequency for word, freq in word_frequencies.items()}\n",
        "\n",
        "        sentence_scores = {}\n",
        "        for sentence in doc.sents:\n",
        "            for word in sentence:\n",
        "                if word.text.lower() in word_frequencies:\n",
        "                    sentence_scores[sentence] = sentence_scores.get(sentence, 0) + word_frequencies[word.text.lower()]\n",
        "\n",
        "        select_length = int(len(list(doc.sents)) * compression_ratio)\n",
        "        summarized_sentences = nlargest(select_length, sentence_scores, key=sentence_scores.get)\n",
        "        summary = ' '.join([sentence.text for sentence in summarized_sentences])\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        print(f\"Error summarizing text: {e}\")\n",
        "        return text\n"
      ],
      "metadata": {
        "id": "UH6AnUTXbNOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ROUGE**"
      ],
      "metadata": {
        "id": "sn8euJNSbctQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "\n",
        "num_samples = 100\n",
        "documents = ds2[\"article\"][:num_samples]\n",
        "reference_summaries = ds3[\"abstract\"][:num_samples]\n",
        "\n",
        "rouge_scores = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n",
        "\n",
        "for doc, ref_summary in tqdm(zip(documents, reference_summaries), total=num_samples):\n",
        "        predicted_summary = summarize_text(doc)\n",
        "        scores = scorer.score(predicted_summary, ref_summary)\n",
        "\n",
        "        rouge_scores[\"rouge1\"].append(scores[\"rouge1\"].fmeasure)\n",
        "        rouge_scores[\"rouge2\"].append(scores[\"rouge2\"].fmeasure)\n",
        "        rouge_scores[\"rougeL\"].append(scores[\"rougeL\"].fmeasure)\n",
        "\n",
        "avg_rouge = {key: sum(values) / num_samples for key, values in rouge_scores.items()}\n",
        "\n",
        "print(\"Average ROUGE Scores:\")\n",
        "for metric, value in avg_rouge.items():\n",
        "    print(f\"{metric.upper()}: {value:.4f}\")"
      ],
      "metadata": {
        "id": "JEP6SPDybPfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BERTScore**"
      ],
      "metadata": {
        "id": "g26Hgj1cbjHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_bertscore_on_summaries(dataset, summarize_text, num_samples=100):\n",
        "    documents = ds1[\"document\"][:num_samples]\n",
        "    reference_summaries = ds1[\"summary\"][:num_samples]\n",
        "\n",
        "    generated_summaries = []\n",
        "\n",
        "    for doc in tqdm(documents, total=num_samples):\n",
        "        predicted_summary = summarize_text(doc)\n",
        "        generated_summaries.append(predicted_summary)\n",
        "\n",
        "    P, R, F1 = score(generated_summaries, reference_summaries, lang=\"en\", verbose=True)\n",
        "\n",
        "    avg_bertscore = F1.mean().item()\n",
        "\n",
        "    return avg_bertscore\n",
        "\n",
        "avg_bertscore = evaluate_bertscore_on_summaries(ds1, summarize_text)\n",
        "\n",
        "print(f\"Average BERTScore: {avg_bertscore:.4f}\")"
      ],
      "metadata": {
        "id": "TKX_Ti0jbWep"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}